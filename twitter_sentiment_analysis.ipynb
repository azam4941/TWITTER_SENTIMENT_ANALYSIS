{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Given Twitter US Airline Sentiment Dataset, which contains data for over 14000 tweets, your task is to predict the sentiment of the tweet i.e. positive, negative or neutral.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A Training dataset csv file with X train and Y train data\n",
    "2. A X test File and you have to predict and submit predictions for this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"twitter_train.csv\")\n",
    "test=pd.read_csv(\"twitter_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=train.to_numpy()\n",
    "testing=test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=training[:,6:8]\n",
    "test_data=testing[:,5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_data[:,1]\n",
    "Y_train=train_data[:,0]\n",
    "\n",
    "X_test=test_data[:,1]\n",
    "Y_test=test_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops=stopwords.words('english')\n",
    "import string \n",
    "punctuations=list(string.punctuation)\n",
    "stops+=punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr=[w for w in X_train if not w in stops]\n",
    "y_tr=[w for w in Y_train if not w in stops]\n",
    "x_ts=[w for w in X_test if not w in stops]\n",
    "y_ts=[w for w in Y_test if not w in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=PorterStemmer()\n",
    "xtr=[st.stem(w) for w in x_tr]\n",
    "xts=[st.stem(w) for w in x_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_document=[]\n",
    "j=0\n",
    "for i in (xtr):\n",
    "    if y_tr[j]==0:\n",
    "        train_document.append((i,\"neg\"))\n",
    "        j+=1\n",
    "    else:\n",
    "        train_document.append((i,\"tru\"))\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_document=[]\n",
    "j=0\n",
    "for i in (xts):\n",
    "    if y_tr[j]==0:\n",
    "        test_document.append((i,\"neg\"))\n",
    "        j+=1\n",
    "    else:\n",
    "        test_document.append((i,\"pos\"))\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tag_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('v'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def clean_reviews(words):\n",
    "    output_word=[]\n",
    "    for w in words:\n",
    "        if w.lower() not in stops:\n",
    "            pos=pos_tag([w])\n",
    "            clean=lemmatizer.lemmatize(w,pos=sample_tag_pos(pos[0][1]))\n",
    "            output_word.append(clean.lower())\n",
    "    return output_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "train_da=[]\n",
    "test_da=[]\n",
    "for i in range(len(train_document)):\n",
    "    a=train_document[i][0]\n",
    "    c=word_tokenize(a)\n",
    "    b=[w for w in c if not w in stops]\n",
    "    train_da.append((b,train_document[i][1]))\n",
    "for i in range(len(test_document)):\n",
    "    a=test_document[i][0]\n",
    "    c=word_tokenize(a)\n",
    "    b=[w for w in c if not w in stops]\n",
    "    test_da.append((b,test_document[i][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents=[(clean_reviews(document1),cat) for document1, cat in train_da]\n",
    "test_documents=[(clean_reviews(document1),cat) for document1, cat in test_da]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word=[]\n",
    "for doc in train_documents:\n",
    "    all_word+=doc[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "frequency=nltk.FreqDist(all_word)  ## how many words are common and how much?\n",
    "common=frequency.most_common(300)          ## here print only 15 highest common words\n",
    "features=[i[0] for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_doc(word):\n",
    "    current_feature={}\n",
    "    words_set=set(word)\n",
    "    for w in features:\n",
    "        current_feature[w]=w not in words_set\n",
    "    return current_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_final_data=[(get_word_doc(doc),category) for doc, category in train_documents]\n",
    "testing_final_data=[(get_word_doc(doc),category) for doc, category in test_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9322404371584699"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "classifier=NaiveBayesClassifier.train(training_final_data)\n",
    "nltk.classify.accuracy(classifier,testing_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "svc=SVC()\n",
    "sklearn_classifier=SklearnClassifier(svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azam ali\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9510928961748634"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_classifier.train(training_final_data)\n",
    "nltk.classify.accuracy(sklearn_classifier,testing_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfd=RandomForestClassifier()\n",
    "classifier_sklearner=SklearnClassifier(rfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azam ali\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False))>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_sklearner.train(training_final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9489071038251367"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_sklearner,testing_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
